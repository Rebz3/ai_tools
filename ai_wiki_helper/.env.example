# Confluence Configuration
CONFLUENCE_URL=
CONFLUENCE_TOKEN=

# LLM Configuration
LLM_BASE_URL=
# Cервисная учетка для работы скрипта
LLM_API_KEY=
LLM_MODEL_NAME=
LLM_TEMPERATURE=
LLM_TOP_P=
LLM_TOP_K=
LLM_MAX_TOKENS=


# Рекомендации по настройке параметров LLM в зависимости от типа тестирования:
# - Для функционального тестирования (точность, детерминированность):
#   TEMPERATURE=0.1-0.3, TOP_P=0.8-0.9, TOP_K=30-50
# - Для эксплоративного тестирования (поиск неочевидных сценариев):
#   TEMPERATURE=0.5-0.7, TOP_P=0.95, TOP_K=50-100
# - Для регрессионного тестирования (стабильность, предсказуемость):
#   TEMPERATURE=0.1-0.2, TOP_P=0.85, TOP_K=20-40
# - Для негативного/краевого тестирования (вариативность сценариев):
#   TEMPERATURE=0.4-0.6, TOP_P=0.9, TOP_K=40-80
# - Для нагрузочного тестирования (минимизация ошибок в логике):
#   TEMPERATURE=0.1-0.2, TOP_P=0.75-0.85, TOP_K=10-30

# Общие советы:
# - TEMPERATURE: 0.1-0.3 для критических систем, 0.5-0.7 для творческих задач.
# - TOP_P и TOP_K: используются для фильтрации маловероятных токенов.
#   Меньшие значения (TOP_P=0.7-0.85, TOP_K=20-40) — для точности,
#   большие (TOP_P=0.9-1, TOP_K=50-100) — для вариативности.
# - Комбинируйте параметры: например, низкая температура + умеренный TOP_P
#   даст стабильные, но не скучные тест-кейсы.
# - Если в выводе текст обрывается, значит необходимо увеличить количество токенов в LLM_MAX_TOKENS.
